networks:
  common:
    name: common

# настройки и конфигурации, общие для всех контейнеров
x-minio-common: &minio-common
  image: quay.io/minio/minio:RELEASE.2022-10-15T19-57-03Z # устанавливаем image
  # команда которая выполняется при запуске контейнера
  # --console-address ":9001" - указываем запустить консоль администратора на порту 9001
  #  http://minio{1...4}/data{1...2}  - указывает ноде адреса других нод,
  # чтобы они могли связаться между собой
  # если сломается одна из нод,
  # остальные ноды это поймут и будут предпринимать действия чтобы сохранить файлы
  command: server --console-address ":9001" http://minio{1...4}/data{1...2}
  environment: # env конфигурация, подробнее: https://github.com/KaymeKaydex/web-2022/tree/go-lab2/tutorials/lab2/golang#поговорим-про-переменные-окружения
    MINIO_ACCESS_KEY: "${S3_MINIO_ACCESS_KEY}" # пароль админа
    MINIO_SECRET_KEY: "${S3_MINIO_SECRET_KEY}" # логин админа
    MINIO_ROOT_USER: "${S3_MINIO_ROOT_USER}"
    MINIO_ROOT_PASSWORD: "${S3_MINIO_ROOT_PASSWORD}"
  expose:
    - "9000" # открыть порты
    - "9001"
    # environment:
  healthcheck: # проверка состояния работоспособности кластера происходит путем выполнения get запроса на http://localhost:9000/minio/health/live
    test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
    interval: 30s
    timeout: 20s
    retries: 3

services:
  minio1:
    <<: *minio-common
    hostname: minio1 # указываем что адрес внутри подсети docker будет http://minio1:9000/ и на порту 9001 для админки соотвественно
    volumes: # куда физически переадресовать* память(в какие разделы) из виртуальной(в докере) на вашу машину. по сути это работает как хард линк на файл/папку
      - data1-1:/data1
      - data1-2:/data2
    networks:
      - common

  minio2: # аналогично ставим 2-4 ноды
    <<: *minio-common
    hostname: minio2
    volumes:
      - data2-1:/data1
      - data2-2:/data2
    networks:
      - common

  minio3:
    <<: *minio-common
    hostname: minio3
    volumes:
      - data3-1:/data1
      - data3-2:/data2
    networks:
      - common

  minio4:
    <<: *minio-common
    hostname: minio4
    volumes:
      - data4-1:/data1
      - data4-2:/data2
    networks:
      - common

  nginx: # наш еще один сервис в подсети docker будет nginx
    container_name: s3-minio
    image: nginx:1.19.2-alpine # исходники сервиса взять отсюда
    hostname: s3-minio # hostname внутри подсети docker будет s3-minio
    volumes:
      # куда физически переадресовать* память(в какие разделы) из виртуальной(в докере) на вашу машину. по сути это работает как хард линк на файл/папку.
      # доступ даем на ro (read only)
      - ./config/nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "9000:9000" # форвардим порт 9000 из нашей локальной сети(localhost) (forward port) на исхоодный(внутри подсети docker)(source port)
      - "9001:9001" # левый порт - порт вашей локальной сети компьютера, таргетный. порт справа - соурс порт - порт внутри подсети докера.
    networks:
      - common
    depends_on: # не запускай nginx пока не запустится весь кластер minio
      minio1:
        condition: service_healthy
      minio2:
        condition: service_healthy
      minio3:
        condition: service_healthy
      minio4:
        condition: service_healthy

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    volumes:
      - ./config/grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yaml
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true
    networks:
      - common
    container_name: grafana

  loki:
    image: grafana/loki:latest
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - common
    container_name: loki

  promtail:
    image:  grafana/promtail:latest
    container_name: promtail
    volumes:
      - ./config/promtail.yml:/etc/promtail/docker-config.yaml
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
    command: -config.file=/etc/promtail/docker-config.yaml
    depends_on:
      - loki
    networks:
      - common

  prometheus:
    image: prom/prometheus
    container_name: prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    restart: unless-stopped
    volumes:
      - ./config/prometheus:/etc/prometheus
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "9090:9090"
    networks:
      - common
  jaeger:
    image: jaegertracing/all-in-one:1.51
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "16686:16686"
      - "14268:14268"
    container_name: jaeger
    networks:
      - common
#    logging:
#      driver: none
## По умолчанию в этой конфигурации используется локальный драйвер docker по умолчанию,
## Для пользовательских томов замените на конфигурацию драйвера тома.
volumes:
  data1-1:
  data1-2:
  data2-1:
  data2-2:
  data3-1:
  data3-2:
  data4-1:
  data4-2: